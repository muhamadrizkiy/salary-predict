{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this\n",
    "\n",
    "Run each cell from top to bottom. \n",
    "View README.md for more infos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: [1, 8, 1, 16, 1, 7, 14, 6, 5, 2, 1, 1, 1, 41]\n",
      "input_dim: 105\n",
      "\n",
      "output_dim: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init global infos\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "inputs = (\n",
    "    (\"age\", (\"continuous\",)), \n",
    "    (\"workclass\", (\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\")), \n",
    "    (\"fnlwgt\", (\"continuous\",)), \n",
    "    (\"education\", (\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\")), \n",
    "    (\"education-num\", (\"continuous\",)), \n",
    "    (\"marital-status\", (\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\", \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\")), \n",
    "    (\"occupation\", (\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\", \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\", \"Armed-Forces\")), \n",
    "    (\"relationship\", (\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\", \"Unmarried\")), \n",
    "    (\"race\", (\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\")), \n",
    "    (\"sex\", (\"Female\", \"Male\")), \n",
    "    (\"capital-gain\", (\"continuous\",)), \n",
    "    (\"capital-loss\", (\"continuous\",)), \n",
    "    (\"hours-per-week\", (\"continuous\",)), \n",
    "    (\"native-country\", (\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\", \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\", \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"))\n",
    ")\n",
    "\n",
    "input_shape = []\n",
    "for i in inputs:\n",
    "    count = len(i[1 ])\n",
    "    input_shape.append(count)\n",
    "input_dim = sum(input_shape)\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"input_dim:\", input_dim)\n",
    "print()\n",
    "\n",
    "\n",
    "outputs = (0, 1)  # (\">50K\", \"<=50K\")\n",
    "output_dim = 2  # len(outputs)\n",
    "print(\"output_dim:\", output_dim)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load and prepare data\n",
    "\n",
    "def isFloat(string):\n",
    "    # credits: http://stackoverflow.com/questions/2356925/how-to-check-whether-string-might-be-type-cast-to-float-in-python\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def find_means_for_continuous_types(X):\n",
    "    means = []\n",
    "    for col in range(len(X[0])):\n",
    "        summ = 0\n",
    "        count = 0.000000000000000000001\n",
    "        for value in X[:, col]:\n",
    "            if isFloat(value): \n",
    "                summ += float(value)\n",
    "                count +=1\n",
    "        means.append(summ/count)\n",
    "    return means\n",
    "\n",
    "def prepare_data(raw_data, means):\n",
    "    \n",
    "    X = raw_data[:, :-1]\n",
    "    y = raw_data[:, -1:]\n",
    "    \n",
    "    # X:\n",
    "    def flatten_persons_inputs_for_model(person_inputs):\n",
    "        global inputs\n",
    "        global input_shape\n",
    "        global input_dim\n",
    "        global means\n",
    "        float_inputs = []\n",
    "\n",
    "        for i in range(len(input_shape)):\n",
    "            features_of_this_type = input_shape[i]\n",
    "            is_feature_continuous = features_of_this_type == 1\n",
    "\n",
    "            if is_feature_continuous:\n",
    "                mean = means[i]\n",
    "                if isFloat(person_inputs[i]):\n",
    "                    scale_factor = 1/(2*mean)  # we prefer inputs mainly scaled from -1 to 1. \n",
    "                    float_inputs.append(float(person_inputs[i])*scale_factor)\n",
    "                else:\n",
    "                    float_inputs.append(mean)\n",
    "            else:\n",
    "                for j in range(features_of_this_type):\n",
    "                    feature_name = inputs[i][1][j]\n",
    "\n",
    "                    if feature_name == person_inputs[i]:\n",
    "                        float_inputs.append(1.)\n",
    "                    else:\n",
    "                        float_inputs.append(0)\n",
    "        return float_inputs\n",
    "    \n",
    "    new_X = []\n",
    "    for person in range(len(X)):\n",
    "        formatted_X = flatten_persons_inputs_for_model(X[person])\n",
    "        new_X.append(formatted_X)\n",
    "    new_X = np.array(new_X)\n",
    "    \n",
    "    # y:\n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == \">50k\":\n",
    "            new_y.append((1, 0))\n",
    "        else:  # y[i] == \"<=50k\":\n",
    "            new_y.append((0, 1))\n",
    "    new_y = np.array(new_y)\n",
    "    \n",
    "    return (new_X, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 32561\n",
      "Test data count: 16281\n",
      "Mean values for data types (if continuous): [38.64358543876172, 0.0, 189664.13459727284, 0.0, 10.078088530363212, 0.0, 0.0, 0.0, 0.0, 0.0, 1079.0676262233324, 87.50231358257237, 40.422382375824085, 0.0, 0.0]\n",
      "Training data percentage that is >50k: 24.0809557446 %\n"
     ]
    }
   ],
   "source": [
    "# Building training and test data\n",
    "\n",
    "training_data = np.genfromtxt('data/adult.data.txt', delimiter=', ', dtype=str, autostrip=True)\n",
    "print(\"Training data count:\", len(training_data))\n",
    "test_data = np.genfromtxt('data/adult.test.txt', delimiter=', ', dtype=str, autostrip=True)\n",
    "print(\"Test data count:\", len(test_data))\n",
    "\n",
    "means = find_means_for_continuous_types(np.concatenate((training_data, test_data), 0))\n",
    "print(\"Mean values for data types (if continuous):\", means)\n",
    "\n",
    "X_train, y_train = prepare_data(training_data, means)\n",
    "X_test, y_test = prepare_data(test_data, means)\n",
    "\n",
    "percent = sum([i[0] for i in y_train])/len(y_train)\n",
    "print(\"Training data percentage that is >50k:\", percent*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data format example:\n",
      "[ 0.36228522  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.89212702  1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.64496357\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          1.          0.          0.\n",
      "  0.          0.49477539  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "\n",
      "In fact, we just crushed the data in such a way that it will optimise the neural network (model). \n",
      "It is crushed according to the `input_shape` variable: \n",
      "    say, if there are 41 native countries in the dataset, there will be 41 input dimensions for the \n",
      "    neural network with a value of 0 for every 41 input node for a given person, except the \n",
      "    node representing the real country of the person which will have a value of 1. For continuous \n",
      "    values, they are normalised to a small float number.\n"
     ]
    }
   ],
   "source": [
    "# Explanation on data format\n",
    "\n",
    "print(\"Training data format example:\")\n",
    "print(X_train[4])  # 4 is a random person, from cuba. \n",
    "print()\n",
    "\n",
    "print(\"In fact, we just crushed the data in such a way that it will optimise the neural network (model). \\n\\\n",
    "It is crushed according to the `input_shape` variable: \\n\\\n",
    "    say, if there are 41 native countries in the dataset, there will be 41 input dimensions for the \\n\\\n",
    "    neural network with a value of 0 for every 41 input node for a given person, except the \\n\\\n",
    "    node representing the real country of the person which will have a value of 1. For continuous \\n\\\n",
    "    values, they are normalised to a small float number.\")\n",
    "\n",
    "for i in X_train:\n",
    "    if len(i) != input_dim:\n",
    "        raise Exception(\n",
    "            \"Every person should have 105 data fields now. {} here.\".format(len(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init model\n",
    "\n",
    "mid_dim = 12\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=mid_dim, activation='sigmoid', input_dim=input_dim))\n",
    "model.add(Dense(output_dim=output_dim, activation='sigmoid', input_dim=mid_dim))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(training_datas, dimension): (32561, 105)\n",
      "Train on 29304 samples, validate on 3257 samples\n",
      "Epoch 0\n",
      "29304/29304 [==============================] - 1s - loss: 0.6432 - acc: 0.6701 - val_loss: 0.5089 - val_acc: 0.7694\n",
      "Epoch 1\n",
      "29304/29304 [==============================] - 0s - loss: 0.4481 - acc: 0.7961 - val_loss: 0.4062 - val_acc: 0.8170\n",
      "Epoch 2\n",
      "29304/29304 [==============================] - 0s - loss: 0.3795 - acc: 0.8294 - val_loss: 0.3630 - val_acc: 0.8388\n",
      "Epoch 3\n",
      "29304/29304 [==============================] - 0s - loss: 0.3523 - acc: 0.8435 - val_loss: 0.3464 - val_acc: 0.8468\n",
      "Epoch 4\n",
      "29304/29304 [==============================] - 0s - loss: 0.3414 - acc: 0.8453 - val_loss: 0.3393 - val_acc: 0.8443\n",
      "Epoch 5\n",
      "29304/29304 [==============================] - 0s - loss: 0.3361 - acc: 0.8469 - val_loss: 0.3352 - val_acc: 0.8434\n",
      "Epoch 6\n",
      "29304/29304 [==============================] - 0s - loss: 0.3328 - acc: 0.8486 - val_loss: 0.3326 - val_acc: 0.8431\n",
      "Epoch 7\n",
      "29304/29304 [==============================] - 0s - loss: 0.3303 - acc: 0.8490 - val_loss: 0.3308 - val_acc: 0.8459\n",
      "Epoch 8\n",
      "29304/29304 [==============================] - 0s - loss: 0.3284 - acc: 0.8502 - val_loss: 0.3290 - val_acc: 0.8440\n",
      "Epoch 9\n",
      "29304/29304 [==============================] - 0s - loss: 0.3270 - acc: 0.8510 - val_loss: 0.3281 - val_acc: 0.8459\n",
      "Epoch 10\n",
      "29304/29304 [==============================] - 0s - loss: 0.3258 - acc: 0.8511 - val_loss: 0.3268 - val_acc: 0.8431\n",
      "Epoch 11\n",
      "29304/29304 [==============================] - 0s - loss: 0.3247 - acc: 0.8518 - val_loss: 0.3258 - val_acc: 0.8431\n",
      "Epoch 12\n",
      "29304/29304 [==============================] - 0s - loss: 0.3239 - acc: 0.8515 - val_loss: 0.3253 - val_acc: 0.8446\n",
      "Epoch 13\n",
      "29304/29304 [==============================] - 0s - loss: 0.3230 - acc: 0.8509 - val_loss: 0.3246 - val_acc: 0.8459\n",
      "Epoch 14\n",
      "29304/29304 [==============================] - 0s - loss: 0.3223 - acc: 0.8522 - val_loss: 0.3246 - val_acc: 0.8471\n",
      "Epoch 15\n",
      "29304/29304 [==============================] - 0s - loss: 0.3216 - acc: 0.8524 - val_loss: 0.3242 - val_acc: 0.8477\n",
      "Epoch 16\n",
      "29304/29304 [==============================] - 0s - loss: 0.3211 - acc: 0.8521 - val_loss: 0.3233 - val_acc: 0.8465\n",
      "Epoch 17\n",
      "29304/29304 [==============================] - 0s - loss: 0.3205 - acc: 0.8522 - val_loss: 0.3233 - val_acc: 0.8471\n",
      "Epoch 18\n",
      "29304/29304 [==============================] - 0s - loss: 0.3201 - acc: 0.8529 - val_loss: 0.3222 - val_acc: 0.8465\n",
      "Epoch 19\n",
      "29304/29304 [==============================] - 0s - loss: 0.3196 - acc: 0.8528 - val_loss: 0.3218 - val_acc: 0.8471\n",
      "Epoch 20\n",
      "29304/29304 [==============================] - 0s - loss: 0.3191 - acc: 0.8535 - val_loss: 0.3216 - val_acc: 0.8471\n",
      "Epoch 21\n",
      "29304/29304 [==============================] - 0s - loss: 0.3187 - acc: 0.8527 - val_loss: 0.3216 - val_acc: 0.8483\n",
      "Epoch 22\n",
      "29304/29304 [==============================] - 0s - loss: 0.3183 - acc: 0.8529 - val_loss: 0.3210 - val_acc: 0.8502\n",
      "Epoch 23\n",
      "29304/29304 [==============================] - 0s - loss: 0.3178 - acc: 0.8535 - val_loss: 0.3207 - val_acc: 0.8517\n",
      "Epoch 24\n",
      "29304/29304 [==============================] - 0s - loss: 0.3175 - acc: 0.8536 - val_loss: 0.3206 - val_acc: 0.8499\n",
      "Epoch 25\n",
      "29304/29304 [==============================] - 0s - loss: 0.3172 - acc: 0.8528 - val_loss: 0.3198 - val_acc: 0.8514\n",
      "Epoch 26\n",
      "29304/29304 [==============================] - 0s - loss: 0.3168 - acc: 0.8535 - val_loss: 0.3202 - val_acc: 0.8511\n",
      "Epoch 27\n",
      "29304/29304 [==============================] - 0s - loss: 0.3164 - acc: 0.8531 - val_loss: 0.3197 - val_acc: 0.8496\n",
      "Epoch 28\n",
      "29304/29304 [==============================] - 0s - loss: 0.3162 - acc: 0.8535 - val_loss: 0.3194 - val_acc: 0.8526\n",
      "Epoch 29\n",
      "29304/29304 [==============================] - 0s - loss: 0.3159 - acc: 0.8535 - val_loss: 0.3194 - val_acc: 0.8526\n",
      "Epoch 30\n",
      "29304/29304 [==============================] - 0s - loss: 0.3154 - acc: 0.8540 - val_loss: 0.3191 - val_acc: 0.8505\n",
      "Epoch 31\n",
      "29304/29304 [==============================] - 0s - loss: 0.3153 - acc: 0.8549 - val_loss: 0.3191 - val_acc: 0.8489\n",
      "Epoch 32\n",
      "29304/29304 [==============================] - 0s - loss: 0.3149 - acc: 0.8538 - val_loss: 0.3185 - val_acc: 0.8514\n",
      "Epoch 33\n",
      "29304/29304 [==============================] - 0s - loss: 0.3146 - acc: 0.8541 - val_loss: 0.3185 - val_acc: 0.8520\n",
      "Epoch 34\n",
      "29304/29304 [==============================] - 0s - loss: 0.3143 - acc: 0.8541 - val_loss: 0.3183 - val_acc: 0.8514\n",
      "Epoch 35\n",
      "29304/29304 [==============================] - 0s - loss: 0.3140 - acc: 0.8539 - val_loss: 0.3176 - val_acc: 0.8511\n",
      "Epoch 36\n",
      "29304/29304 [==============================] - 0s - loss: 0.3139 - acc: 0.8539 - val_loss: 0.3175 - val_acc: 0.8508\n",
      "Epoch 37\n",
      "29304/29304 [==============================] - 0s - loss: 0.3135 - acc: 0.8545 - val_loss: 0.3183 - val_acc: 0.8508\n",
      "Epoch 38\n",
      "29304/29304 [==============================] - 0s - loss: 0.3132 - acc: 0.8545 - val_loss: 0.3172 - val_acc: 0.8511\n",
      "Epoch 39\n",
      "29304/29304 [==============================] - 0s - loss: 0.3131 - acc: 0.8546 - val_loss: 0.3173 - val_acc: 0.8514\n",
      "Epoch 40\n",
      "29304/29304 [==============================] - 0s - loss: 0.3127 - acc: 0.8547 - val_loss: 0.3172 - val_acc: 0.8496\n",
      "Epoch 41\n",
      "29304/29304 [==============================] - 0s - loss: 0.3125 - acc: 0.8552 - val_loss: 0.3168 - val_acc: 0.8514\n",
      "Epoch 42\n",
      "29304/29304 [==============================] - 0s - loss: 0.3121 - acc: 0.8548 - val_loss: 0.3163 - val_acc: 0.8508\n",
      "Epoch 43\n",
      "29304/29304 [==============================] - 0s - loss: 0.3119 - acc: 0.8552 - val_loss: 0.3170 - val_acc: 0.8499\n",
      "Epoch 44\n",
      "29304/29304 [==============================] - 0s - loss: 0.3117 - acc: 0.8550 - val_loss: 0.3164 - val_acc: 0.8514\n",
      "Epoch 45\n",
      "29304/29304 [==============================] - 0s - loss: 0.3116 - acc: 0.8551 - val_loss: 0.3160 - val_acc: 0.8517\n",
      "Epoch 46\n",
      "29304/29304 [==============================] - 0s - loss: 0.3113 - acc: 0.8549 - val_loss: 0.3160 - val_acc: 0.8520\n",
      "Epoch 47\n",
      "29304/29304 [==============================] - 0s - loss: 0.3110 - acc: 0.8554 - val_loss: 0.3161 - val_acc: 0.8505\n",
      "Epoch 48\n",
      "29304/29304 [==============================] - 0s - loss: 0.3107 - acc: 0.8552 - val_loss: 0.3166 - val_acc: 0.8496\n",
      "Epoch 49\n",
      "29304/29304 [==============================] - 0s - loss: 0.3105 - acc: 0.8553 - val_loss: 0.3156 - val_acc: 0.8517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd99e316828>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "print(\"(training_datas, dimension):\", X_train.shape)\n",
    "# model.fit(new_X_train, y_train, nb_epoch=3, batch_size=16, show_accuracy=True, verbose=2)\n",
    "model.fit(X_train, y_train, nb_epoch=50, batch_size=128, validation_split=0.1, show_accuracy=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16281/16281 [==============================] - 0s     \n",
      "\n",
      "Test Results for 16281 test entries on which we did not trained the neural network.\n",
      "\n",
      "Keras evaluation result: 0.31364969079\n",
      "Percentage right: 85.33259627786991%.\n",
      "Error: 14.66740372213009%.\n",
      "\n",
      "Confusion matrix:\n",
      "[[22911  2904]\n",
      " [ 1809  4937]]\n",
      "Confusion matrix, percentage of data:\n",
      "[[ 70.36331808   8.918645  ]\n",
      " [  5.55572618  15.16231074]]\n",
      "Confusion matrix interpretation:\n",
      " [['true negative' 'false negative']\n",
      " ['false positive' 'true positive']]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate training\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1, show_accuracy=True)\n",
    "print(\"\\nTest Results for {} test entries \\\n",
    "on which we did not trained the neural network.\\n\".format(len(X_test)))\n",
    "\n",
    "print(\"Keras evaluation result:\", score[0])\n",
    "print(\"Percentage right: {}%.\".format(score[1]*100))\n",
    "print(\"Error: {}%.\\n\".format((1-score[1])*100))\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    confusion_matrix = np.array([\n",
    "        [0, 0], \n",
    "        [0, 0]\n",
    "    ])\n",
    "    pred = model.predict(X_train)\n",
    "    for i in range(len(pred)):\n",
    "        prediction = pred[i]\n",
    "        if prediction[0]>prediction[1]:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "\n",
    "        expected = y_train[i][0]\n",
    "\n",
    "        confusion_matrix[prediction][expected] += 1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "confusion_matrix = evaluate_model(model, X_test, y_test)\n",
    "confusion_matrix_interpretation = np.array([\n",
    "        [\"true negative\", \"false negative\"], \n",
    "        [\"false positive\", \"true positive\"]\n",
    "    ])\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "print(\"Confusion matrix, percentage of data:\")\n",
    "print(confusion_matrix*100/sum(confusion_matrix.flatten()))\n",
    "print(\"Confusion matrix interpretation:\\n\", confusion_matrix_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
